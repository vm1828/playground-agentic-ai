{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f6ea461",
   "metadata": {},
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8735c95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# %pip install langchain==0.3.27\n",
    "# %pip install jupyterlab==4.4.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00988689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import uuid\n",
    "import operator\n",
    "from typing import TypedDict, Annotated\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image\n",
    "\n",
    "__import__(\"pysqlite3\")\n",
    "import sys\n",
    "\n",
    "sys.modules[\"sqlite3\"] = sys.modules.pop(\"pysqlite3\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.tools import tool, create_retriever_tool\n",
    "from langchain_core.messages import AnyMessage, AIMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.memory import MemorySaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4189a7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY = os.environ[\"GOOGLE_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094cb000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELS\n",
    "model_gemini_2_0_flash = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    google_api_key=GOOGLE_API_KEY,\n",
    "    temperature=0,\n",
    "    max_tokens=100,\n",
    "    max_retries=1,\n",
    ")\n",
    "\n",
    "# model_embedding_001 = GoogleGenerativeAIEmbeddings(\n",
    "#     model=\"models/embedding-001\", google_api_key=GOOGLE_API_KEY\n",
    "# )\n",
    "\n",
    "model_embedding_001 = HuggingFaceEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\", model_kwargs={\"device\": \"cpu\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c48b06a",
   "metadata": {},
   "source": [
    "# ReAct Agent Example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d8625c",
   "metadata": {},
   "source": [
    "## Create agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d4be79",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def find_sum(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Add two integers and return their sum.\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def find_product(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Multiply two integers and return their product.\n",
    "    \"\"\"\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32a2786",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_tools = [find_sum, find_product]\n",
    "\n",
    "system_prompt = SystemMessage(\n",
    "    \"\"\"\n",
    "    You are a mathematician who solves math problems using tools only. \n",
    "    Do not solve any problem yourself â€” always use the available tools to find the solution.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "agent_graph = create_react_agent(\n",
    "    model=model_gemini_2_0_flash, prompt=system_prompt, tools=agent_tools\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8026bc6c",
   "metadata": {},
   "source": [
    "## Invoke agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2b6746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example 1\n",
    "# inputs = {\"messages\": [(\"user\", \"what is the sum of 2 and 3 ?\")]}\n",
    "# result = agent_graph.invoke(inputs)\n",
    "\n",
    "# print(f\"Agent returned : {result['messages'][-1].content} \\n\")\n",
    "# print(\"Step by Step execution : \")\n",
    "# for message in result[\"messages\"]:\n",
    "#     print(message.pretty_repr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dbc56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example 2\n",
    "# inputs = {\"messages\": [(\"user\", \"What is 3 multipled by 2 and 5 + 1 ?\")]}\n",
    "# result = agent_graph.invoke(inputs)\n",
    "\n",
    "# print(f\"Agent returned : {result['messages'][-1].content} \\n\")\n",
    "# print(\"Step by Step execution : \")\n",
    "# for message in result[\"messages\"]:\n",
    "#     print(message.pretty_repr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499f4a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example 3\n",
    "# inputs = {\"messages\": [(\"user\", \"what is the sum of 2.1 and 3.7 ?\")]}\n",
    "# result = agent_graph.invoke(inputs)\n",
    "\n",
    "# print(f\"Agent returned : {result['messages'][-1].content} \\n\")\n",
    "# print(\"Step by Step execution : \")\n",
    "# for message in result[\"messages\"]:\n",
    "#     print(message.pretty_repr())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e899d517",
   "metadata": {},
   "source": [
    "## Debug agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dedb175",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_graph_debug = create_react_agent(\n",
    "    model=model_gemini_2_0_flash, prompt=system_prompt, tools=agent_tools, debug=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff9853e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = {\"messages\": [(\"user\", \"what is the sum of 2 and 3 ?\")]}\n",
    "# result = agent_graph_debug.invoke(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a790cffa",
   "metadata": {},
   "source": [
    "# Product Q&A Chatbot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55eaf4b",
   "metadata": {},
   "source": [
    "## Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea57dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_price_df = pd.read_csv(\"data/smartphone_prices.csv\")\n",
    "product_price_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7931b62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"./data/smartphone_descriptions.pdf\")\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18408dcc",
   "metadata": {},
   "source": [
    "## Tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da26191",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_smartphone_price(smartphone_name: str) -> int:\n",
    "    \"\"\"\n",
    "    Returns the price of a smartphone by name (case-insensitive substring match).\n",
    "    If no match is found, returns -1.\n",
    "    \"\"\"\n",
    "    pattern = f\"^{smartphone_name.strip()}\"\n",
    "    matches = product_price_df[\n",
    "        product_price_df[\"Name\"].str.contains(pattern, case=False, na=False)\n",
    "    ]\n",
    "    if matches.empty:\n",
    "        return -1\n",
    "    return int(matches[\"Price\"].iloc[0])\n",
    "\n",
    "\n",
    "print(get_smartphone_price.invoke(\"zenith\"))\n",
    "print(get_smartphone_price.invoke(\"asdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2e3285",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=256)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "feature_store = Chroma.from_documents(documents=splits, embedding=model_embedding_001)\n",
    "\n",
    "get_product_features = create_retriever_tool(\n",
    "    feature_store.as_retriever(search_kwargs={\"k\": 1}),\n",
    "    name=\"Get_Product_Features\",\n",
    "    description=\"\"\"\n",
    "    This store contains details about smartphones. It lists the available smartphones\n",
    "    and their features including camera, memory, storage, design and advantages\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "feature_store.as_retriever().invoke(\"Tell me about the Zenith One\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a929ce",
   "metadata": {},
   "source": [
    "## Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eca84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = SystemMessage(\n",
    "    \"\"\"\n",
    "    You are professional chatbot that answers questions about smartphones sold by your company.\n",
    "    To answer questions about smartphones, you will ONLY use the available tools and NOT your own memory.\n",
    "    You will handle small talk and greetings by producing professional responses.\n",
    "    \"\"\"\n",
    ")\n",
    "checkpointer = MemorySaver()  # conversation memory\n",
    "\n",
    "product_QnA_agent = create_react_agent(\n",
    "    model=model_gemini_2_0_flash,\n",
    "    tools=[get_smartphone_price, get_product_features],\n",
    "    prompt=system_prompt,\n",
    "    debug=False,\n",
    "    checkpointer=checkpointer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6ef48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To maintain memory, each request should be in the context of a thread.\n",
    "# Each user conversation will use a separate thread ID\n",
    "config = {\"configurable\": {\"thread_id\": uuid.uuid4()}}\n",
    "\n",
    "inputs = {\n",
    "    \"messages\": [HumanMessage(\"What are the features and pricing for Zenith One?\")]\n",
    "}\n",
    "\n",
    "# Use streaming to print responses as the agent  does the work.\n",
    "# This is an alternate way to stream agent responses without waiting for the agent to finish\n",
    "for stream in product_QnA_agent.stream(inputs, config, stream_mode=\"values\"):\n",
    "    message = stream[\"messages\"][-1]\n",
    "    if isinstance(message, tuple):\n",
    "        print(message)\n",
    "    else:\n",
    "        message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588e57fa",
   "metadata": {},
   "source": [
    "## Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb94cedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This simulates the conversation between the user and the Agentic chatbot\n",
    "user_inputs = [\n",
    "    \"Hello\",\n",
    "    \"I am looking to buy a smartphone\",\n",
    "    \"Give me a list of available smartphone names\",\n",
    "    \"Tell me about the features of Zenith One\",\n",
    "    \"How much does it cost?\",\n",
    "    \"Give me similar information about TitanMax\",\n",
    "    \"What info do you have on Nimbus ?\",\n",
    "    \"Thanks for the help\",\n",
    "]\n",
    "\n",
    "# Create a new thread\n",
    "config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "\n",
    "for input in user_inputs:\n",
    "    time.sleep(1)\n",
    "    print(f\"----------------------------------------\\nUSER : {input}\")\n",
    "    user_message = {\"messages\": [HumanMessage(input)]}\n",
    "    ai_response = product_QnA_agent.invoke(user_message, config=config)\n",
    "    print(f\"AGENT : {ai_response['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b95edaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversation memory by user\n",
    "def execute_prompt(user, config, prompt):\n",
    "    inputs = {\"messages\": [(\"user\", prompt)]}\n",
    "    ai_response = product_QnA_agent.invoke(inputs, config=config)\n",
    "    print(f\"\\n{user}: {ai_response['messages'][-1].content}\")\n",
    "\n",
    "\n",
    "# Create different session threads for 2 users\n",
    "config_1 = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "config_2 = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "\n",
    "# Test both threads\n",
    "execute_prompt(\"USER 1\", config_1, \"Tell me about the features of  Zenith\")\n",
    "execute_prompt(\"USER 2\", config_2, \"Tell me about the features of Nimbus Mini\")\n",
    "execute_prompt(\"USER 1\", config_1, \"What is its price ?\")\n",
    "execute_prompt(\"USER 2\", config_2, \"What is its price ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d47588",
   "metadata": {},
   "source": [
    "# Orders Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4cd559",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcdaea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_orders_df = pd.read_csv(\"data/smartphone_orders.csv\")\n",
    "product_orders_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dd7788",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba12914",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_order_details(order_id: str) -> str:\n",
    "    \"\"\"\n",
    "    This function returns details about a smartphone order, given an order ID\n",
    "    It performs an exact match between the input order id and available order ids\n",
    "    If a match is found, it returns products (smartphones) ordered, quantity ordered and delivery date.\n",
    "    If there is NO match found, it returns -1\n",
    "    \"\"\"\n",
    "    matches = product_orders_df[product_orders_df[\"Order ID\"] == order_id]\n",
    "\n",
    "    if matches.empty:\n",
    "        return -1\n",
    "    else:\n",
    "        return matches.iloc[0].to_dict()\n",
    "\n",
    "\n",
    "print(get_order_details.invoke(\"ORD-5821\"))\n",
    "print(get_order_details.invoke(\"ORD-8510\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d25866",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def update_quantity(order_id: str, new_quantity: int) -> bool:\n",
    "    \"\"\"\n",
    "    This function updates the quantity of products (smartphones) ordered for a given order Id.\n",
    "    It there are no matching orders, it returns -1.\n",
    "    \"\"\"\n",
    "    matches = product_orders_df[product_orders_df[\"Order ID\"] == order_id]\n",
    "\n",
    "    if matches.empty:\n",
    "        return -1\n",
    "    else:\n",
    "        product_orders_df.loc[\n",
    "            product_orders_df[\"Order ID\"] == order_id, \"Quantity Ordered\"\n",
    "        ] = new_quantity\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fdbfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to test comment out @tool decorator\n",
    "# print(get_order_details(\"ORD-5821\"))\n",
    "# print(update_quantity(\"ORD-5821\", 20))\n",
    "# print(get_order_details(\"ORD-9022\"))\n",
    "# print(update_quantity(\"ORD-9022\", 50))\n",
    "# product_orders_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6979bd68",
   "metadata": {},
   "source": [
    "## Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464c0b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An Agent State class that keeps the state of the agent while it answers a query\n",
    "class OrdersAgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]\n",
    "\n",
    "\n",
    "# An Agent class\n",
    "class OrdersAgent:\n",
    "\n",
    "    def __init__(self, model, tools, system_prompt, debug):\n",
    "\n",
    "        self.system_prompt = system_prompt\n",
    "        self.debug = debug\n",
    "\n",
    "        # Setup the graph for the agent manually\n",
    "        agent_graph = StateGraph(OrdersAgentState)\n",
    "        agent_graph.add_node(\"orders_llm\", self.call_llm)\n",
    "        agent_graph.add_node(\"orders_tools\", self.call_tools)\n",
    "        agent_graph.add_conditional_edges(\n",
    "            \"orders_llm\", self.is_tool_call, {True: \"orders_tools\", False: END}\n",
    "        )\n",
    "        agent_graph.add_edge(\"orders_tools\", \"orders_llm\")\n",
    "        agent_graph.set_entry_point(\"orders_llm\")\n",
    "        self.memory = MemorySaver()  # chat memory\n",
    "        self.agent_graph = agent_graph.compile(\n",
    "            checkpointer=self.memory\n",
    "        )  # compile the graph\n",
    "\n",
    "        # Setup tools\n",
    "        self.tools = {tool.name: tool for tool in tools}\n",
    "        if self.debug:\n",
    "            print(\"\\nTools loaded :\", self.tools)\n",
    "        self.model = model.bind_tools(tools)  # attach tools to model\n",
    "\n",
    "    # Call the LLM with the messages to get next action/result\n",
    "    def call_llm(self, state: OrdersAgentState):\n",
    "\n",
    "        messages = state[\"messages\"]\n",
    "        if self.system_prompt:\n",
    "            messages = [SystemMessage(content=self.system_prompt)] + messages\n",
    "\n",
    "        result = self.model.invoke(\n",
    "            messages\n",
    "        )  # invoke the model with the message history\n",
    "        if self.debug:\n",
    "            print(f\"\\nLLM Returned : {result}\")\n",
    "\n",
    "        return {\"messages\": [result]}  # Return the LLM output\n",
    "\n",
    "    # Check if the next action is a tool call.\n",
    "    def is_tool_call(self, state: OrdersAgentState):\n",
    "        last_message = state[\"messages\"][-1]\n",
    "        if len(last_message.tool_calls) > 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    # Execute the tool requested with the given parameters\n",
    "    def call_tools(self, state: OrdersAgentState):\n",
    "        tool_calls = state[\"messages\"][-1].tool_calls\n",
    "        results = []\n",
    "\n",
    "        for tool in tool_calls:\n",
    "            # Handle tool missing error\n",
    "            if not tool[\"name\"] in self.tools:\n",
    "                print(f\"Unknown tool name {tool}\")\n",
    "                result = \"Invalid tool found. Please retry\"\n",
    "            else:\n",
    "                result = self.tools[tool[\"name\"]].invoke(tool[\"args\"])\n",
    "\n",
    "            # append results to the list of tool results\n",
    "            results.append(\n",
    "                ToolMessage(\n",
    "                    tool_call_id=tool[\"id\"], name=tool[\"name\"], content=str(result)\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if self.debug:\n",
    "                print(f\"\\nTools returned {results}\")\n",
    "            # return tool results\n",
    "            return {\"messages\": results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c076d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "    You are professional chatbot that manages orders for smartphones sold by our company.\n",
    "    The tools allow for retrieving order details as well as update order quantity.\n",
    "    Do NOT reveal information about other orders than the one requested.\n",
    "    You will handle small talk and greetings by producing professional responses.\n",
    "    \"\"\"\n",
    "\n",
    "orders_agent = OrdersAgent(\n",
    "    model_gemini_2_0_flash,\n",
    "    [get_order_details, update_quantity],\n",
    "    system_prompt,\n",
    "    debug=False,\n",
    ")\n",
    "\n",
    "# Visualize the Agent\n",
    "Image(orders_agent.agent_graph.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baae5668",
   "metadata": {},
   "source": [
    "## Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a1ade5",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_inputs = [\n",
    "    \"How are you doing?\",\n",
    "    \"Please show me the details of the order ORD-5821\",\n",
    "    \"Can you add one more of that laptop to the order? \",\n",
    "    \"Can you show me the details again ? \",\n",
    "    \"What about order ORD-9999 ?\",\n",
    "    \"Bye\",\n",
    "]\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}  # create a new thread\n",
    "\n",
    "for input in user_inputs:\n",
    "    print(f\"----------------------------------------\\nUSER : {input}\")\n",
    "    user_message = {\"messages\": [HumanMessage(input)]}  # format the user message\n",
    "    ai_response = orders_agent.agent_graph.invoke(\n",
    "        user_message, config=config\n",
    "    )  # get response from the agent\n",
    "    print(f\"\\nAGENT : {ai_response['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942147f3",
   "metadata": {},
   "source": [
    "# Reflection-based Summary Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee66fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_MESSAGE = \"DECISION: STOP\"\n",
    "CONTINUE_MESSAGE = \"DECISION: CONTINUE\"\n",
    "\n",
    "summarizer_prompt = \"\"\"\n",
    "You are a document summarizer. Summarize provided text in under 100 words.\n",
    "If human or ai gives review, feedback or critique, revise your previous summary accordingly and provide improved summary.\n",
    "\"\"\"\n",
    "\n",
    "reviewer_prompt = f\"\"\"\n",
    "You are a reviewer grading summaries.\n",
    "\n",
    "1. Read through the messages and find the original text and the last summary.\n",
    "2. Give brief, clear feedback about accuracy, coverage, and brevity.\n",
    "3. You MUST always output at least one sentence of feedback.\n",
    "4. At the end of your feedback, output exactly one of the following lines:\n",
    "{STOP_MESSAGE}\n",
    "{CONTINUE_MESSAGE}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece28692",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummaryAgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]\n",
    "\n",
    "\n",
    "class SummaryAgent:\n",
    "\n",
    "    def __init__(self, model, summarizer_prompt, reviewer_prompt, debug):\n",
    "        self.model = model\n",
    "        self.summarizer_prompt = summarizer_prompt\n",
    "        self.reviewer_prompt = reviewer_prompt\n",
    "        self.debug = debug\n",
    "\n",
    "        # Setup the graph for the agent manually\n",
    "        agent_graph = StateGraph(SummaryAgentState)\n",
    "        agent_graph.add_node(\"summarizer\", self.generate_summary)\n",
    "        agent_graph.add_node(\"reviewer\", self.review_summary)\n",
    "        agent_graph.add_conditional_edges(\n",
    "            \"summarizer\", self.should_continue, {True: \"reviewer\", False: END}\n",
    "        )\n",
    "        agent_graph.add_edge(\"reviewer\", \"summarizer\")\n",
    "        agent_graph.set_entry_point(\"summarizer\")\n",
    "        self.memory = MemorySaver()  # chat memory\n",
    "        self.agent_graph = agent_graph.compile(\n",
    "            checkpointer=self.memory\n",
    "        )  # compile the graph\n",
    "\n",
    "    def generate_summary(self, state: SummaryAgentState):\n",
    "        messages = state[\"messages\"]\n",
    "        messages = [SystemMessage(content=self.summarizer_prompt)] + messages\n",
    "\n",
    "        # Invoke summarizer with the message history\n",
    "        result = self.model.invoke(messages)\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"--- Prompt for summarizer ---\")\n",
    "            for m in messages:\n",
    "                print(f\"{m.type.upper()}: {m.content}\")\n",
    "            print(\"-----------------------------\")\n",
    "            print(\n",
    "                f\"==============\\nSummarizer output:\\n{result.content}\\n==============\\n\"\n",
    "            )\n",
    "        tagged_result = AIMessage(content=f\"[SUMMARY]\\n{result.content}\")\n",
    "        return {\"messages\": [tagged_result]}\n",
    "\n",
    "    def review_summary(self, state: SummaryAgentState):\n",
    "        messages = state[\"messages\"]\n",
    "        messages = [SystemMessage(content=self.reviewer_prompt)] + messages\n",
    "\n",
    "        # Invoke reviewer with the message history\n",
    "        result = self.model.invoke(messages)\n",
    "\n",
    "        if self.debug:\n",
    "            print(f\"*************\\nReviewer output:\\n{result.content}\\n*************\\n\")\n",
    "        tagged_result = AIMessage(content=f\"[REVIEW]\\n{result.content}\")\n",
    "        return {\"messages\": [tagged_result]}\n",
    "\n",
    "    def should_continue(self, state: SummaryAgentState):\n",
    "        total_reviews = len(state[\"messages\"]) / 2\n",
    "        last_review = next(\n",
    "            (\n",
    "                m\n",
    "                for m in reversed(state[\"messages\"])\n",
    "                if m.content.startswith(\"[REVIEW]\")\n",
    "            ),\n",
    "            \"\",\n",
    "        )\n",
    "        if self.debug:\n",
    "            print(f\"Iteration number: {total_reviews}\\n\")\n",
    "            print(f\"Last review: {last_review}\")\n",
    "\n",
    "        # Return if 2 iterations are completed or summary is satisfactory.\n",
    "        # Each iteration has 2 messages\n",
    "        if len(state[\"messages\"]) > 4 or STOP_MESSAGE in last_review:\n",
    "            return False\n",
    "        else:\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c6831b",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_chatbot = SummaryAgent(\n",
    "    model_gemini_2_0_flash, summarizer_prompt, reviewer_prompt, debug=True\n",
    ")\n",
    "\n",
    "Image(summary_chatbot.agent_graph.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a419a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"./data/renewable_energy.pdf\")\n",
    "docs = loader.load()\n",
    "source_content = docs[0].page_content.replace(\"\\n\", \" \")\n",
    "print(source_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2eea00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "\n",
    "messages=[HumanMessage(content=source_content)]\n",
    "result=summary_chatbot.agent_graph.invoke({\"messages\":messages},config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47eb10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_chatbot = SummaryAgent(\n",
    "    model_gemini_2_0_flash, summarizer_prompt, reviewer_prompt, debug=False\n",
    ")\n",
    "\n",
    "\n",
    "user_inputs = [\n",
    "    source_content,\n",
    "    \"Can you rewrite the review making it more scientific?\",\n",
    "    \"Can you make it even shorter?\",\n",
    "]\n",
    "\n",
    "# Create a new thread\n",
    "config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "\n",
    "# Given the number of iterations, this will take a long time.\n",
    "for input in user_inputs:\n",
    "    print(f\"----------------------------------------\\nUSER : {input}\")\n",
    "    # Format the user message\n",
    "    user_message = {\"messages\": [HumanMessage(input)]}\n",
    "    # Get response from the agent\n",
    "    ai_response = summary_chatbot.agent_graph.invoke(\n",
    "        user_message, config=config\n",
    "    )\n",
    "    # Print the response\n",
    "    print(f\"\\nAGENT : {ai_response['messages'][-1].content}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
